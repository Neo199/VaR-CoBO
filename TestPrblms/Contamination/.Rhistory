# Define item parameters (intercepts and slopes)
alpha_0 <- runif(M, -1, 1)   # Intercepts
alpha   <- runif(M, 0.5, 2)  # Slopes
# Generate binary responses X based on logistic probability
X <- matrix(0, nrow = N, ncol = M)
for (j in 1:M) {
theta <- 1 / (1 + exp(-(alpha_0[j] + alpha[j] * Z)))  # Logistic function
X[, j] <- rbinom(N, 1, theta)  # Sample binary responses
}
# Convert to data frame for rstanarm
X_df <- as.data.frame(X)
X_df$subject <- 1:N
X_long <- reshape2::melt(X_df, id.vars = "subject", variable.name = "item", value.name = "response")
# Fit a hierarchical logistic regression model
fit <- stan_glmer(response ~ (1 | subject) + (1 | item),
data = X_long,
family = binomial(link = "logit"),
chains = 4, iter = 2000)
# Print summary
print(fit)
# Plot latent trait vs response for each item
df <- data.frame(subject = rep(1:N, each = M),
item = rep(1:M, times = N),
Z = rep(Z, each = M),
X = as.vector(X_long$response))
ggplot(df, aes(x = Z, y = X, color = as.factor(item))) +
geom_point() +
facet_wrap(~subject) +  # Add subject as a facet
labs(title = "Latent Trait vs Observed Responses", x = "Latent Trait (Z)", y = "Response (X)")
# Plot the logistic curve showing latent trait (Z) vs predicted probability of response
ggplot(data.frame(Z, theta), aes(x = Z, y = theta)) +
geom_line(color = "blue") +
labs(title = "Latent Trait vs Predicted Probability",
x = "Latent Trait (Z)", y = "Predicted Probability of Response") +
theme_minimal()
data.frame(Z, theta)
df
data.frame(Z, theta, X)
# Extract posterior samples of the fixed effects and random effects
posterior_samples <- as.array(fit)
# Extract posterior samples of the latent traits (Z)
latent_trait_samples <- posterior_samples[, "Z", , drop = FALSE]  # Adjust this based on the model output
posterior_samples
X
X_new <- c(1,0,0,0,1)
# Posterior predictive checks
posterior_pred <- posterior_predict(fit, newdata = X_new)
X_new <-data.frame(c(1,0,0,0,1))
X_new
X_new <-data.frame(matrix(c(1,0,0,0,1), nrow= 1)
)
X_new
# Posterior predictive checks
posterior_pred <- posterior_predict(fit, newdata = X_new)
X_long
# Posterior predictive checks
posterior_pred <- posterior_predict(fit, newdata = X_long)
Z
posterior_predict()
posterior_pred
View(posterior_pred)
# Extract posterior samples for the latent variables
posterior_samples <- posterior_samples(fit, pars = "latent_variable")
# Fit a 2PL model using the ltm package
model <- ltm(X ~ z1)  # z1 is the latent trait for each subject
# Load necessary library
library(ltm)
install.packages("ltm")
# Fit a 2PL model using the ltm package
model <- ltm(X ~ z1)  # z1 is the latent trait for each subject
# Load necessary library
library(ltm)
# Fit a 2PL model using the ltm package
model <- ltm(X ~ z1)  # z1 is the latent trait for each subject
# Print summary of the model
summary(model)
z1
View(model)
X
model$coefficients
model$GH
Contamination <- function(x, runlength, seed) {
# Initialize outputs
FnVar <- NA
FnGrad <- NA
FnGradCov <- NA
ConstraintGrad <- NA
ConstraintGradCov <- NA
n <- length(x)
# Input validation
if (length(x) != n || any(x > 1) || any(x < 0) || runlength <= 0 || seed <= 0 || seed != round(seed)) {
cat(sprintf("x has %d elements, elements of x are binary,
runlength should be positive and real, seed should be a positive integer.\n", n))
return(list(fn = NA, constraint = NA, ConstraintCov = NA))
} else {
# Parameters
nGen <- runlength         # number of independent generations
u <- x                    # prevention binary decision variable
X <- matrix(0, n, nGen)   # fraction contaminated at each stage for each generation
epsilon <- rep(0.05, n)   # error probability
p <- rep(0.1, n)          # proportion limit
cost <- rep(1, n)         # cost for prevention at stage i
# Beta parameters for initial contamination, contamination rate, restoration rate
initialAlpha <- 1
initialBeta <- 30
contamAlpha <- 1
contamBeta <- 17 / 3
restoreAlpha <- 1
restoreBeta <- 3 / 7
# Set random seed
set.seed(seed)
# Generate initial fraction of contamination
initialX <- rbeta(nGen, initialAlpha, initialBeta)
# Generate rates of contamination and restoration
Lambda <- matrix(rbeta(n * nGen, contamAlpha, contamBeta), n, nGen)
Gamma <- matrix(rbeta(n * nGen, restoreAlpha, restoreBeta), n, nGen)
# Determine contamination fractions
X[1, ] <- Lambda[1, ] * (1 - u[1]) * (1 - initialX) + (1 - Gamma[1, ] * u[1]) * initialX
for (i in 2:n) {
X[i, ] <- Lambda[i, ] * (1 - u[i]) * (1 - X[i - 1, ]) + (1 - Gamma[i, ] * u[i]) * X[i - 1, ]
}
# Limit and cost of contamination control
limit <- 1 - epsilon
fn <- sum(cost * u)
# Constraint checking
con <- matrix(0, nGen, n)
for (j in 1:nGen) {
con[j, ] <- X[, j] <= p
}
le <- sum(rowSums(con) == n)
constraint <- rep(0, n)
for (k in 1:n) {
constraint[k] <- (sum(con[, k]) / runlength) - limit[k]
}
ConstraintCov <- cov(con)
return(list(fn = fn, constraint = constraint, ConstraintCov = ConstraintCov))
}
}
x <- c(1, 0, 1)  # Binary decision vector
runlength <- 1000
seed <- 42
result <- Contamination(x, runlength, seed)
cat("Cost (fn):", result$fn, "\n")
cat("Constraints:", result$constraint, "\n")
cat("Constraint Covariance Matrix:\n")
print(result$ConstraintCov)
contamination_prob <- function(x, n_samples, seed) {
# Declare gamma factor (Lagrange constants)
gamma <- 1
# Find the total number of input samples
num_inputs <- nrow(x)
out <- numeric(num_inputs)
# Iterate over each input sample
for (i in 1:num_inputs) {
# Run contamination study
contamination_result <- Contamination(x[i, ], n_samples, seed)
cost <- contamination_result$fn
constraint <- contamination_result$constraint
# Compute total output
out[i] <- cost - sum(gamma * constraint)
}
return(out)
}
# Define input matrix (binary variables)
x <- matrix(c(0, 1, 1, 0, 1, 0), nrow = 3, byrow = TRUE)
# Parameters
n_samples <- 1000
seed <- 42
# Compute contamination probabilities
out <- contamination_prob(x, n_samples, seed)
print(out)
# Define input matrix (binary variables)
x <- matrix(c(0, 1, 1, 0, 1, 0), nrow = 3, byrow = TRUE)
# Parameters
n_samples <- 1000
seed <- 42
# Compute contamination probabilities
out <- contamination_prob(x, n_samples, seed)
print(out)
# Define input matrix (binary variables)
x <- matrix(c(0, 1, 1, 0, 1, 0), nrow = 3, byrow = TRUE)
# Parameters
n_samples <- 1000
seed <- 5
# Compute contamination probabilities
out <- contamination_prob(x, n_samples, seed)
print(out)
x <- c(1, 0, 1)  # Binary decision vector
runlength <- 1000
seed <- 42
result <- Contamination(x, runlength, seed)
cat("Cost (fn):", result$fn, "\n")
cat("Constraints:", result$constraint, "\n")
100
100
100
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/P2Compute/Demo/LTA.R")
Z
X
model$coefficients
model$GH
df
z
Z
X
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/PRBOCS_TS_stan_GA.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/PRBOCS_TS_stan_GA.R")
x_current <- rbinom(n = n_vars, size = 1, prob = theta_current)
stat_model <- function(theta) {
con_y <- thompson_sam(theta, bayesian_model = bayesian_model, removed_columns, data)
}
ga_model <- function(theta) {
f <- -(stat_model(theta))
# pen <- sqrt(.Machine$double.xmax)  # penalty term
# penalty1 <- max(c1(x_current),0)*pen
# penalty2 <- max(c2(x_current),0)*pen
(f) # - penalty1 - penalty2)            # fitness function value
}
GA <- ga(type = "real-valued", fitness = ga_model, nBits = n_vars, lower = 0, upper = 1,
popSize = 100, maxiter = 1000, run = 100)
summary(GA)
plot (GA)
# Evaluate model objective at the new evaluation point
# Truncate x_new to the first n_vars elements
x_new <- x_new <- GA@solution
x_new <- matrix(x_new, nrow=1, ncol=n_vars)
cat("New evaluation point", x_new, "\n")
# browser()
#Append new point to existing x_vals
x_vals_updated <- rbind(xTrain, x_new)
# Evaluate model objective at the new evaluation point
y_new <- model(x_new, seed)
x_new <- matrix(x_new, nrow = 1, ncol = n_vars)
x_new_in_comb <- order_effects(x_new, order)
x_new_in <- x_new_in_comb$xTrain_in
expected_val <- GA@solution
cat("expected_val", expected_val, "\n")
x_new <- rbinom(length(expected_val), 1, expected_val)
# Evaluate model objective at the new evaluation point
# Truncate x_new to the first n_vars elements
x_new <- x_new <- GA@solution
x_new <- matrix(x_new, nrow=1, ncol=n_vars)
cat("New evaluation point", x_new, "\n")
expected_val <- GA@solution
cat("expected_val", expected_val, "\n")
x_new <- rbinom(length(expected_val), 1, expected_val)
# Evaluate model objective at the new evaluation point
# Truncate x_new to the first n_vars elements
x_new <- x_new[1:n_vars]
x_new <- matrix(x_new, nrow=1, ncol=n_vars)
cat("New evaluation point", x_new, "\n")
GA <- ga(type = "real-valued", fitness = ga_model, nBits = n_vars, lower = 0, upper = 1,
popSize = 100, maxiter = 1000, run = 100)
summary(GA)
plot (GA)
expected_val <- GA@solution
cat("expected_val", expected_val, "\n")
GA@solution
GA <- ga(type = "real-valued", fitness = ga_model, nBits = n_vars, lower = 0, upper = 1,
popSize = 100, maxiter = 1000, run = 100)
summary(GA)
x_current <- rbinom(n = n_vars, size = 1, prob = theta_current)
stat_model <- function(theta) {
con_y <- thompson_sam(theta, bayesian_model = bayesian_model, removed_columns, data)
}
ga_model <- function(theta) {
f <- -(stat_model(theta))
# pen <- sqrt(.Machine$double.xmax)  # penalty term
# penalty1 <- max(c1(x_current),0)*pen
# penalty2 <- max(c2(x_current),0)*pen
(f) # - penalty1 - penalty2)            # fitness function value
}
GA <- ga(type = "real-valued", fitness = ga_model, nBits = n_vars, lower = 0, upper = 1,
popSize = 100, maxiter = 1000, run = 100)
summary(GA)
View(ga_model)
GA <- ga(type = "real-valued", fitness = ga_model, lower = rep(0, n_vars), upper = rep(1, n_vars),
popSize = 100, maxiter = 1000, run = 100)
summary(GA)
best_solution <- GA@solution
print(best_solution)
GA <- ga(type = "real-valued", fitness = ga_model, lower = rep(0, n_vars), upper = rep(1, n_vars),
popSize = 100, maxiter = 1000, run = 100)
plot(GA)
summary(GA)
expected_val <- GA@solution
cat("expected_val", expected_val, "\n")
x_new <- rbinom(length(expected_val), 1, expected_val)
# Evaluate model objective at the new evaluation point
# Truncate x_new to the first n_vars elements
x_new <- x_new[1:n_vars]
x_new <- matrix(x_new, nrow=1, ncol=n_vars)
cat("New evaluation point", x_new, "\n")
# browser()
#Append new point to existing x_vals
x_vals_updated <- rbind(xTrain, x_new)
# Evaluate model objective at the new evaluation point
y_new <- model(x_new, seed)
x_new <- matrix(x_new, nrow = 1, ncol = n_vars)
x_new_in_comb <- order_effects(x_new, order)
x_new_in <- x_new_in_comb$xTrain_in
data_new <- data.frame(y = y_new, x_new_in)
data <- rbind(data, data_new)
hs_ss_sd <-sd(x_vals_updated)
theta_current <- expected_val
optim_result[t,] <- expected_val
#Running the Ling reg to train on constrained data
is_constant <- apply(data, 2, function(x) length(unique(x)) == 1)
data_reduced <- data[, !is_constant]
# Save the names of the columns that were removed
removed_columns <- names(data)[is_constant]
#Horseshoe scaling using piironen and vehtari 2017
p0 <- n_vars/2
n <- nrow(xTrain_in)
p <- ncol(xTrain_in)
slab_scale<-sqrt(0.3/p0)*hs_ss_sd
#global scale without sigma, as the scaling by sigma
#is done inside stan_glm
global_scale<-(p0/(p-p0))/sqrt(n)
bayesian_model <- stan_glm(y ~ ., data = data_reduced, family = gaussian(),
prior=hs(global_scale=global_scale, slab_scale=slab_scale),
prior_intercept = normal(0,1), iter = 1000)
for (t in 1:evalBudget) {
x_current <- rbinom(n = n_vars, size = 1, prob = theta_current)
stat_model <- function(theta) {
con_y <- thompson_sam(theta, bayesian_model = bayesian_model, removed_columns, data)
}
ga_model <- function(theta) {
f <- -(stat_model(theta))
# pen <- sqrt(.Machine$double.xmax)  # penalty term
# penalty1 <- max(c1(x_current),0)*pen
# penalty2 <- max(c2(x_current),0)*pen
(f) # - penalty1 - penalty2)            # fitness function value
}
GA <- ga(type = "real-valued", fitness = ga_model, lower = rep(0, n_vars), upper = rep(1, n_vars),
popSize = 100, maxiter = 1000, run = 100)
plot(GA)
summary(GA)
expected_val <- GA@solution
cat("expected_val", expected_val, "\n")
x_new <- rbinom(length(expected_val), 1, expected_val)
# Evaluate model objective at the new evaluation point
# Truncate x_new to the first n_vars elements
x_new <- x_new[1:n_vars]
x_new <- matrix(x_new, nrow=1, ncol=n_vars)
cat("New evaluation point", x_new, "\n")
# browser()
#Append new point to existing x_vals
x_vals_updated <- rbind(xTrain, x_new)
# Evaluate model objective at the new evaluation point
y_new <- model(x_new, seed)
x_new <- matrix(x_new, nrow = 1, ncol = n_vars)
x_new_in_comb <- order_effects(x_new, order)
x_new_in <- x_new_in_comb$xTrain_in
data_new <- data.frame(y = y_new, x_new_in)
data <- rbind(data, data_new)
hs_ss_sd <-sd(x_vals_updated)
theta_current <- expected_val
optim_result[t,] <- expected_val
#Running the Ling reg to train on constrained data
is_constant <- apply(data, 2, function(x) length(unique(x)) == 1)
data_reduced <- data[, !is_constant]
# Save the names of the columns that were removed
removed_columns <- names(data)[is_constant]
#Horseshoe scaling using piironen and vehtari 2017
p0 <- n_vars/2
n <- nrow(xTrain_in)
p <- ncol(xTrain_in)
slab_scale<-sqrt(0.3/p0)*hs_ss_sd
#global scale without sigma, as the scaling by sigma
#is done inside stan_glm
global_scale<-(p0/(p-p0))/sqrt(n)
bayesian_model <- stan_glm(y ~ ., data = data_reduced, family = gaussian(),
prior=hs(global_scale=global_scale, slab_scale=slab_scale),
prior_intercept = normal(0,1), iter = 1000)
}
# Plot objective function values versus iterations for normal optimization
plot(1:nrow(data), data$y , type = "l", xlab = "Iterations",
ylab = "Objective Function Value", main = "Objective Function vs Iterations",
col = "red", xlim = c(1, nrow(data)))
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/PRBOCS_TS_stan_GA.R")
data
x_vals
y_vals
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
# Plot BOCS-GA data
plot(1:nrow(bocsga_data), bocsga_data$y, type = "l", xlab = "Iterations",
ylab = "Objective Function Value", main = "Objective Function vs Iterations",
col = "red", xlim = c(1, max(nrow(bocsga_data), nrow(bocssa_data), nrow(bocssdp_data))),
ylim = c(1, max(nrow(bocsga_data), nrow(bocssa_data), nrow(bocssdp_data))))
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
setwd("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
gc()
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
closeAllConnections()
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/BOCS_TS_stan_GA_contan.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
browser()
browser()
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
results
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/CBO_constraints_TS/STAN/Optim/PRBOCS_TS_stan3.R")
source("~/Projects:Codes/PhD-Compute/R code/CBO_constraints_TS/STAN/Optim/PRBOCS_TS_stan3.R")
source("~/Projects:Codes/PhD-Compute/R code/CBO_constraints_TS/STAN/Optim/PRBOCS_TS_stan3.R")
data
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
install.packages("psych")
install.packages("ompr")
install.packages("lpSolve")
install.packages("ompr.roi")
install.packages("ggplot2")
install.packages("ROI")
install.packages("GPfit")
install.packages("rstanarm")
install.packages("rstan")
install.packages("GA")
install.packages("ompr.roi")
install.packages("ggplot2")
install.packages("ROI")
install.packages("GPfit")
install.packages("rstanarm")
install.packages("rstan")
install.packages("GA")
install.packages("lpSolve")
install.packages("rstanarm")
install.packages("rstan")
install.packages("GA")
install.packages("lpSolve")
install.packages("GPfit")
install.packages("lpSolve")
install.packages("GPfit")
install.packages("GPfit")
install.packages("GA")
source("~/Contamination/sample_models.R")
source("~/Contamination/Contamination.R")
library(GA)
source("~/sample_models.R")
source("~/Contamination.R")
getwd()
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/BOCS_SDP_Contan.R")
data
source("~/Projects:Codes/PhD-Compute/R code/TestPrblms/Contamination/Pr_SS_cont.R")
source("~/Projects:Codes/P2Compute/LTA_Stan.R")
source("~/Projects:Codes/P2Compute/LTA_Stan.R")
install.packages("rstan")
library(rstan)
set.seed(123)
N <- 10  # Number of subjects
M <- 5   # Number of items
# Generate latent trait Z ~ N(0,1)
Z <- rnorm(N, mean = 0, sd = 1)
# Define item parameters (difficulty & discrimination)
alpha_0 <- runif(M, -1, 1)   # Intercepts (difficulty)
alpha   <- runif(M, 0.5, 2)  # Slopes (discrimination)
# Generate binary responses using logistic function
X <- matrix(0, nrow = N, ncol = M)
for (j in 1:M) {
prob <- 1 / (1 + exp(-(alpha_0[j] + alpha[j] * Z)))  # Logistic probability
X[, j] <- rbinom(N, 1, prob)  # Sample responses
}
# Convert to long format for Stan
data_list <- list(
N = N,
M = M,
X = X
)
stan_code <- "
data {
int<lower=1> N;  // Number of subjects
int<lower=1> M;  // Number of items
int<lower=0, upper=1> X[N, M];  // Binary response matrix
}
parameters {
vector[N] Z;  // Latent trait (person ability)
vector[M] alpha_0;  // Item intercepts
vector<lower=0>[M] alpha;  // Item slopes (discrimination)
}
model {
// Priors
Z ~ normal(0, 1);  // Standard normal prior for latent traits
alpha_0 ~ normal(0, 1);  // Weakly informative prior for difficulty
alpha ~ lognormal(0, 1);  // Constrain slopes to be positive
// Likelihood
for (i in 1:N) {
for (j in 1:M) {
X[i, j] ~ bernoulli_logit(alpha_0[j] + alpha[j] * Z[i]);
}
}
}
"
# Compile and fit the model
fit <- stan(model_code = stan_code, data = data_list,
iter = 2000, chains = 4, warmup = 1000, thin = 2)
# Compile and fit the model
fit <- stan(model_code = stan_code, data = data_list,
iter = 2000, chains = 4, warmup = 1000, thin = 2)
